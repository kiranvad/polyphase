 Main Python Loaded 
2020-08-12 16:15:13,161	WARNING scripts.py:320 -- The --redis-port argument will be deprecated soon. Please use --port instead.
2020-08-12 16:15:13,161	INFO scripts.py:395 -- Using IP address 10.116.28.8 for this node.
2020-08-12 16:15:13,165	INFO resource_spec.py:212 -- Starting Ray with 36.18 GiB memory available for workers and up to 18.09 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2020-08-12 16:15:13,391	WARNING services.py:923 -- Redis failed to start, retrying now.
2020-08-12 16:15:13,791	INFO services.py:1165 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2020-08-12 16:15:13,878	INFO scripts.py:425 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='10.116.28.8:6379' --redis-password='cf408e60-6796-4916-ab3c-bb1acca8b82e'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='cf408e60-6796-4916-ab3c-bb1acca8b82e')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop
2020-08-12 16:15:43,202	INFO scripts.py:468 -- Using IP address 10.116.28.9 for this node.
2020-08-12 16:15:43,207	INFO resource_spec.py:212 -- Starting Ray with 42.19 GiB memory available for workers and up to 18.1 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2020-08-12 16:15:43,262	INFO scripts.py:477 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
2020-08-12 16:15:50,720	INFO scripts.py:468 -- Using IP address 10.116.28.10 for this node.
2020-08-12 16:15:50,725	INFO resource_spec.py:212 -- Starting Ray with 42.19 GiB memory available for workers and up to 18.1 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2020-08-12 16:15:50,787	INFO scripts.py:477 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
2020-08-12 16:15:53,184	INFO scripts.py:468 -- Using IP address 10.116.28.11 for this node.
2020-08-12 16:15:53,190	INFO resource_spec.py:212 -- Starting Ray with 42.19 GiB memory available for workers and up to 18.1 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2020-08-12 16:15:53,245	INFO scripts.py:477 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
2020-08-12 16:15:58,161	INFO scripts.py:468 -- Using IP address 10.116.28.12 for this node.
2020-08-12 16:15:58,166	INFO resource_spec.py:212 -- Starting Ray with 42.19 GiB memory available for workers and up to 18.1 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2020-08-12 16:15:58,220	INFO scripts.py:477 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0812 16:16:05.474238 21544 21544 global_state_accessor.cc:25] Redis server address = 10.116.28.8:6379, is test flag = 0
I0812 16:16:05.475741 21544 21544 redis_client.cc:141] RedisClient connected.
I0812 16:16:05.483826 21544 21544 redis_gcs_client.cc:88] RedisGcsClient Connected.
I0812 16:16:05.484719 21544 21544 service_based_gcs_client.cc:75] ServiceBasedGcsClient Connected.
2020-08-12 16:16:08,358	WARNING worker.py:1047 -- The actor or task with ID 6106f8ac5afc91ccffffffff0100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:10.116.28.12: 1.000000}, {CPU: 20.000000}, {memory: 42.187500 GiB}, {object_store_memory: 12.451172 GiB}. In total there are 1 pending tasks and 0 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
